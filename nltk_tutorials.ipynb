{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('all') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Muad'Dib\",\n",
       " 'learned',\n",
       " 'rapidly',\n",
       " 'because',\n",
       " 'his',\n",
       " 'first',\n",
       " 'training',\n",
       " 'was',\n",
       " 'in',\n",
       " 'how',\n",
       " 'to',\n",
       " 'learn',\n",
       " '.',\n",
       " 'And',\n",
       " 'the',\n",
       " 'first',\n",
       " 'lesson',\n",
       " 'of',\n",
       " 'all',\n",
       " 'was',\n",
       " 'the',\n",
       " 'basic',\n",
       " 'trust',\n",
       " 'that',\n",
       " 'he',\n",
       " 'could',\n",
       " 'learn',\n",
       " '.',\n",
       " 'It',\n",
       " \"'s\",\n",
       " 'shocking',\n",
       " 'to',\n",
       " 'find',\n",
       " 'how',\n",
       " 'many',\n",
       " 'people',\n",
       " 'do',\n",
       " 'not',\n",
       " 'believe',\n",
       " 'they',\n",
       " 'can',\n",
       " 'learn',\n",
       " ',',\n",
       " 'and',\n",
       " 'how',\n",
       " 'many',\n",
       " 'more',\n",
       " 'believe',\n",
       " 'learning',\n",
       " 'to',\n",
       " 'be',\n",
       " 'difficult',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_string = \"\"\"\n",
    "... Muad'Dib learned rapidly because his first training was in how to learn.\n",
    "... And the first lesson of all was the basic trust that he could learn.\n",
    "... It's shocking to find how many people do not believe they can learn,\n",
    "... and how many more believe learning to be difficult.\"\"\"\n",
    "ml_tokenize = nltk.word_tokenize(example_string)\n",
    "ml_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigrams, Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Muad'Dib\", 'learned'),\n",
       " ('learned', 'rapidly'),\n",
       " ('rapidly', 'because'),\n",
       " ('because', 'his'),\n",
       " ('his', 'first'),\n",
       " ('first', 'training'),\n",
       " ('training', 'was'),\n",
       " ('was', 'in'),\n",
       " ('in', 'how'),\n",
       " ('how', 'to'),\n",
       " ('to', 'learn'),\n",
       " ('learn', '.'),\n",
       " ('.', 'And'),\n",
       " ('And', 'the'),\n",
       " ('the', 'first'),\n",
       " ('first', 'lesson'),\n",
       " ('lesson', 'of'),\n",
       " ('of', 'all'),\n",
       " ('all', 'was'),\n",
       " ('was', 'the'),\n",
       " ('the', 'basic'),\n",
       " ('basic', 'trust'),\n",
       " ('trust', 'that'),\n",
       " ('that', 'he'),\n",
       " ('he', 'could'),\n",
       " ('could', 'learn'),\n",
       " ('learn', '.'),\n",
       " ('.', 'It'),\n",
       " ('It', \"'s\"),\n",
       " (\"'s\", 'shocking'),\n",
       " ('shocking', 'to'),\n",
       " ('to', 'find'),\n",
       " ('find', 'how'),\n",
       " ('how', 'many'),\n",
       " ('many', 'people'),\n",
       " ('people', 'do'),\n",
       " ('do', 'not'),\n",
       " ('not', 'believe'),\n",
       " ('believe', 'they'),\n",
       " ('they', 'can'),\n",
       " ('can', 'learn'),\n",
       " ('learn', ','),\n",
       " (',', 'and'),\n",
       " ('and', 'how'),\n",
       " ('how', 'many'),\n",
       " ('many', 'more'),\n",
       " ('more', 'believe'),\n",
       " ('believe', 'learning'),\n",
       " ('learning', 'to'),\n",
       " ('to', 'be'),\n",
       " ('be', 'difficult'),\n",
       " ('difficult', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.bigrams(ml_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Muad'Dib\", 'learned', 'rapidly'),\n",
       " ('learned', 'rapidly', 'because'),\n",
       " ('rapidly', 'because', 'his'),\n",
       " ('because', 'his', 'first'),\n",
       " ('his', 'first', 'training'),\n",
       " ('first', 'training', 'was'),\n",
       " ('training', 'was', 'in'),\n",
       " ('was', 'in', 'how'),\n",
       " ('in', 'how', 'to'),\n",
       " ('how', 'to', 'learn'),\n",
       " ('to', 'learn', '.'),\n",
       " ('learn', '.', 'And'),\n",
       " ('.', 'And', 'the'),\n",
       " ('And', 'the', 'first'),\n",
       " ('the', 'first', 'lesson'),\n",
       " ('first', 'lesson', 'of'),\n",
       " ('lesson', 'of', 'all'),\n",
       " ('of', 'all', 'was'),\n",
       " ('all', 'was', 'the'),\n",
       " ('was', 'the', 'basic'),\n",
       " ('the', 'basic', 'trust'),\n",
       " ('basic', 'trust', 'that'),\n",
       " ('trust', 'that', 'he'),\n",
       " ('that', 'he', 'could'),\n",
       " ('he', 'could', 'learn'),\n",
       " ('could', 'learn', '.'),\n",
       " ('learn', '.', 'It'),\n",
       " ('.', 'It', \"'s\"),\n",
       " ('It', \"'s\", 'shocking'),\n",
       " (\"'s\", 'shocking', 'to'),\n",
       " ('shocking', 'to', 'find'),\n",
       " ('to', 'find', 'how'),\n",
       " ('find', 'how', 'many'),\n",
       " ('how', 'many', 'people'),\n",
       " ('many', 'people', 'do'),\n",
       " ('people', 'do', 'not'),\n",
       " ('do', 'not', 'believe'),\n",
       " ('not', 'believe', 'they'),\n",
       " ('believe', 'they', 'can'),\n",
       " ('they', 'can', 'learn'),\n",
       " ('can', 'learn', ','),\n",
       " ('learn', ',', 'and'),\n",
       " (',', 'and', 'how'),\n",
       " ('and', 'how', 'many'),\n",
       " ('how', 'many', 'more'),\n",
       " ('many', 'more', 'believe'),\n",
       " ('more', 'believe', 'learning'),\n",
       " ('believe', 'learning', 'to'),\n",
       " ('learning', 'to', 'be'),\n",
       " ('to', 'be', 'difficult'),\n",
       " ('be', 'difficult', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(ml_tokenize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Muad'Dib\", 'learned', 'rapidly', 'because', 'his', 'first', 'training'),\n",
       " ('learned', 'rapidly', 'because', 'his', 'first', 'training', 'was'),\n",
       " ('rapidly', 'because', 'his', 'first', 'training', 'was', 'in'),\n",
       " ('because', 'his', 'first', 'training', 'was', 'in', 'how'),\n",
       " ('his', 'first', 'training', 'was', 'in', 'how', 'to'),\n",
       " ('first', 'training', 'was', 'in', 'how', 'to', 'learn'),\n",
       " ('training', 'was', 'in', 'how', 'to', 'learn', '.'),\n",
       " ('was', 'in', 'how', 'to', 'learn', '.', 'And'),\n",
       " ('in', 'how', 'to', 'learn', '.', 'And', 'the'),\n",
       " ('how', 'to', 'learn', '.', 'And', 'the', 'first'),\n",
       " ('to', 'learn', '.', 'And', 'the', 'first', 'lesson'),\n",
       " ('learn', '.', 'And', 'the', 'first', 'lesson', 'of'),\n",
       " ('.', 'And', 'the', 'first', 'lesson', 'of', 'all'),\n",
       " ('And', 'the', 'first', 'lesson', 'of', 'all', 'was'),\n",
       " ('the', 'first', 'lesson', 'of', 'all', 'was', 'the'),\n",
       " ('first', 'lesson', 'of', 'all', 'was', 'the', 'basic'),\n",
       " ('lesson', 'of', 'all', 'was', 'the', 'basic', 'trust'),\n",
       " ('of', 'all', 'was', 'the', 'basic', 'trust', 'that'),\n",
       " ('all', 'was', 'the', 'basic', 'trust', 'that', 'he'),\n",
       " ('was', 'the', 'basic', 'trust', 'that', 'he', 'could'),\n",
       " ('the', 'basic', 'trust', 'that', 'he', 'could', 'learn'),\n",
       " ('basic', 'trust', 'that', 'he', 'could', 'learn', '.'),\n",
       " ('trust', 'that', 'he', 'could', 'learn', '.', 'It'),\n",
       " ('that', 'he', 'could', 'learn', '.', 'It', \"'s\"),\n",
       " ('he', 'could', 'learn', '.', 'It', \"'s\", 'shocking'),\n",
       " ('could', 'learn', '.', 'It', \"'s\", 'shocking', 'to'),\n",
       " ('learn', '.', 'It', \"'s\", 'shocking', 'to', 'find'),\n",
       " ('.', 'It', \"'s\", 'shocking', 'to', 'find', 'how'),\n",
       " ('It', \"'s\", 'shocking', 'to', 'find', 'how', 'many'),\n",
       " (\"'s\", 'shocking', 'to', 'find', 'how', 'many', 'people'),\n",
       " ('shocking', 'to', 'find', 'how', 'many', 'people', 'do'),\n",
       " ('to', 'find', 'how', 'many', 'people', 'do', 'not'),\n",
       " ('find', 'how', 'many', 'people', 'do', 'not', 'believe'),\n",
       " ('how', 'many', 'people', 'do', 'not', 'believe', 'they'),\n",
       " ('many', 'people', 'do', 'not', 'believe', 'they', 'can'),\n",
       " ('people', 'do', 'not', 'believe', 'they', 'can', 'learn'),\n",
       " ('do', 'not', 'believe', 'they', 'can', 'learn', ','),\n",
       " ('not', 'believe', 'they', 'can', 'learn', ',', 'and'),\n",
       " ('believe', 'they', 'can', 'learn', ',', 'and', 'how'),\n",
       " ('they', 'can', 'learn', ',', 'and', 'how', 'many'),\n",
       " ('can', 'learn', ',', 'and', 'how', 'many', 'more'),\n",
       " ('learn', ',', 'and', 'how', 'many', 'more', 'believe'),\n",
       " (',', 'and', 'how', 'many', 'more', 'believe', 'learning'),\n",
       " ('and', 'how', 'many', 'more', 'believe', 'learning', 'to'),\n",
       " ('how', 'many', 'more', 'believe', 'learning', 'to', 'be'),\n",
       " ('many', 'more', 'believe', 'learning', 'to', 'be', 'difficult'),\n",
       " ('more', 'believe', 'learning', 'to', 'be', 'difficult', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(ml_tokenize, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output = I love the book This is a great fit shoes'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "I love the book,\n",
    "This is a great book,\n",
    "The fit is great,\n",
    "I love the shoes\n",
    "\"\"\"\n",
    "\n",
    "\"output = I love the book This is a great fit shoes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification:\n",
    "    book = \"Book\"\n",
    "    clothing = \"clothing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_x = [\"I love the book book\", \"This is a great book\", \"The fit is great\", \"I love the shoes\"]\n",
    "items_y = [Classification.book, Classification.book, Classification.clothing ,Classification.clothing]\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "vectors = vectorizer.fit_transform(items_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['book', 'fit', 'great', 'is', 'love', 'shoes', 'the', 'this'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 1, 0, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_vectors = vectorizer.fit_transform(items_x)\n",
    "train_x_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_vetcor = vectorizer.transform([\"I love cloth shoes\"])\n",
    "train_y_vetcor.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel=\"linear\")\n",
    "clf_svm.fit(train_x_vectors, items_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y_vetcor.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['clothing'], dtype='<U8')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.predict(train_y_vetcor.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get feature of the vectors ['book' 'book book' 'fit' 'fit is' 'great' 'great book' 'is' 'is great'\n",
      " 'love' 'love the' 'shoes' 'the' 'the book' 'the fit' 'the shoes' 'this'\n",
      " 'this is']\n",
      "COrresponding vectors [[1 1 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0]\n",
      " [1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer_1 = CountVectorizer(binary=True, ngram_range=(1,2))\n",
    "vectors_2 = vectorizer_1.fit_transform(items_x)\n",
    "print(\"Get feature of the vectors\", vectorizer_1.get_feature_names_out())\n",
    "print(\"COrresponding vectors\", vectors_2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.12/site-packages (3.8.11)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load the installed model \"en_core_web_sm\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.41792074, -0.12858528,  0.603985  ,  0.31298488,  0.09007233,\n",
       "       -0.11989744,  0.39673483,  0.07185103, -0.00645579, -0.4149039 ,\n",
       "       -0.11226006,  0.33252087,  0.15535668,  0.72004575, -0.07367045,\n",
       "        0.4896722 , -0.8699103 ,  0.22766164, -0.35624713,  0.53306913,\n",
       "       -0.2745465 , -0.03423366, -0.5905246 , -0.3520129 , -0.2927323 ,\n",
       "       -0.22683659,  0.28768843, -0.17548355,  0.6970314 ,  0.6824442 ,\n",
       "       -0.3286007 ,  0.2898847 ,  0.01300885, -0.3285484 , -0.09801619,\n",
       "       -0.05026024, -0.2941342 ,  0.1262176 ,  0.19776848,  0.42000493,\n",
       "        0.36292848,  0.3754    , -0.05160491,  0.30536956,  0.12460868,\n",
       "       -0.17939512,  0.24318793,  0.56805944,  0.31520343, -0.31471115,\n",
       "       -0.5594399 ,  0.3090236 ,  0.28320062,  0.1531665 ,  0.49387175,\n",
       "       -0.6399284 , -0.11522272, -0.06607658, -0.319882  , -0.2775209 ,\n",
       "       -0.02956295, -0.15553303, -0.32715458, -0.4263641 ,  0.19591092,\n",
       "       -0.16091776, -0.10451844, -0.4711995 ,  0.3924827 , -0.09106594,\n",
       "       -0.30631152,  0.11083996, -0.20357181,  0.03595719, -0.31223795,\n",
       "       -0.14229086,  0.20569101, -0.56051004,  0.35587817, -0.4665182 ,\n",
       "       -0.21391317, -0.38363725, -0.14120093, -0.19159457,  0.1159984 ,\n",
       "        0.9089037 ,  0.2810241 , -0.3781145 ,  0.30834877,  0.04219019,\n",
       "       -0.38709143, -0.5884499 ,  0.4679869 , -0.36385208, -0.53693444,\n",
       "        0.10668622], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"This is a text\")\n",
    "doc.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'text']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"This is a text\")\n",
    "# Token texts\n",
    "[token.text for token in doc]\n",
    "# ['This', 'is', 'a', 'text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spans\n",
    "### Accessing spans\n",
    "Span indices are exclusive. So doc[2:4] is a span starting at token 2, up to – but not including! – token 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a text'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"This is a text\")\n",
    "span = doc[2:4]\n",
    "span.text\n",
    "# 'a text'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a span manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love New york\n",
      "Type of doc <class 'spacy.tokens.doc.Doc'>\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Span\n",
    "# Create a doc object \n",
    "doc = nlp(\"I love New york\")\n",
    "print(doc), \n",
    "print(\"Type of doc\", type(doc))\n",
    "\n",
    "## Span for new york with lable geopolitical (GPE)\n",
    "span = Span(doc, 2, 4, label=\"GPE\")\n",
    "span.text\n",
    "print(span.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS (parts of speech) tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sagarguttal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"Muad'Dib\", 'NN')]\n",
      "[('learned', 'VBN')]\n",
      "[('rapidly', 'RB')]\n",
      "[('because', 'IN')]\n",
      "[('his', 'PRP$')]\n",
      "[('first', 'RB')]\n",
      "[('training', 'NN')]\n",
      "[('was', 'VBD')]\n",
      "[('in', 'IN')]\n",
      "[('how', 'WRB')]\n",
      "[('to', 'TO')]\n",
      "[('learn', 'NN')]\n",
      "[('.', '.')]\n",
      "[('And', 'CC')]\n",
      "[('the', 'DT')]\n",
      "[('first', 'RB')]\n",
      "[('lesson', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('all', 'DT')]\n",
      "[('was', 'VBD')]\n",
      "[('the', 'DT')]\n",
      "[('basic', 'JJ')]\n",
      "[('trust', 'NN')]\n",
      "[('that', 'IN')]\n",
      "[('he', 'PRP')]\n",
      "[('could', 'MD')]\n",
      "[('learn', 'NN')]\n",
      "[('.', '.')]\n",
      "[('It', 'PRP')]\n",
      "[(\"'s\", 'POS')]\n",
      "[('shocking', 'VBG')]\n",
      "[('to', 'TO')]\n",
      "[('find', 'VB')]\n",
      "[('how', 'WRB')]\n",
      "[('many', 'JJ')]\n",
      "[('people', 'NNS')]\n",
      "[('do', 'VB')]\n",
      "[('not', 'RB')]\n",
      "[('believe', 'VB')]\n",
      "[('they', 'PRP')]\n",
      "[('can', 'MD')]\n",
      "[('learn', 'NN')]\n",
      "[(',', ',')]\n",
      "[('and', 'CC')]\n",
      "[('how', 'WRB')]\n",
      "[('many', 'JJ')]\n",
      "[('more', 'RBR')]\n",
      "[('believe', 'VB')]\n",
      "[('learning', 'VBG')]\n",
      "[('to', 'TO')]\n",
      "[('be', 'VB')]\n",
      "[('difficult', 'JJ')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "example_string = \"\"\"\n",
    "... Muad'Dib learned rapidly because his first training was in how to learn.\n",
    "... And the first lesson of all was the basic trust that he could learn.\n",
    "... It's shocking to find how many people do not believe they can learn,\n",
    "... and how many more believe learning to be difficult.\"\"\"\n",
    "ml_tokenize = nltk.word_tokenize(example_string)\n",
    "\n",
    "\n",
    "for token in ml_tokenize:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tokens : ['Jerry', 'eats', 'a', 'banana']\n",
      "[('Jerry', 'NN')]\n",
      "[('eats', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('banana', 'NN')]\n",
      "[('eats', 'NNS')]\n",
      "[('Jerry', 'NNP'), ('eats', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "## Disadavatages of POS tag in Nltk\n",
    "sent = \"Jerry eats a banana\"\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "print(\"All tokens :\", tokens)\n",
    "\n",
    "for token in tokens:\n",
    "    print(nltk.pos_tag([token])) \n",
    "\n",
    "###ˇ“eats” is tagged as NNS (plural noun) instead of a verb (VBZ)\n",
    "print(nltk.pos_tag([\"eats\"])) # Context Loss When Tagging Word-by-Word\n",
    "print(nltk.pos_tag([\"Jerry\", \"eats\"])) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantage:\n",
    "\n",
    "NLTK POS tagger performs poorly without sentence-level context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['i',\n",
       "  'me',\n",
       "  'my',\n",
       "  'myself',\n",
       "  'we',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'you',\n",
       "  \"you're\",\n",
       "  \"you've\",\n",
       "  \"you'll\",\n",
       "  \"you'd\",\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  'he',\n",
       "  'him',\n",
       "  'his',\n",
       "  'himself',\n",
       "  'she',\n",
       "  \"she's\",\n",
       "  'her',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'it',\n",
       "  \"it's\",\n",
       "  'its',\n",
       "  'itself',\n",
       "  'they',\n",
       "  'them',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'themselves',\n",
       "  'what',\n",
       "  'which',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'this',\n",
       "  'that',\n",
       "  \"that'll\",\n",
       "  'these',\n",
       "  'those',\n",
       "  'am',\n",
       "  'is',\n",
       "  'are',\n",
       "  'was',\n",
       "  'were',\n",
       "  'be',\n",
       "  'been',\n",
       "  'being',\n",
       "  'have',\n",
       "  'has',\n",
       "  'had',\n",
       "  'having',\n",
       "  'do',\n",
       "  'does',\n",
       "  'did',\n",
       "  'doing',\n",
       "  'a',\n",
       "  'an',\n",
       "  'the',\n",
       "  'and',\n",
       "  'but',\n",
       "  'if',\n",
       "  'or',\n",
       "  'because',\n",
       "  'as',\n",
       "  'until',\n",
       "  'while',\n",
       "  'of',\n",
       "  'at',\n",
       "  'by',\n",
       "  'for',\n",
       "  'with',\n",
       "  'about',\n",
       "  'against',\n",
       "  'between',\n",
       "  'into',\n",
       "  'through',\n",
       "  'during',\n",
       "  'before',\n",
       "  'after',\n",
       "  'above',\n",
       "  'below',\n",
       "  'to',\n",
       "  'from',\n",
       "  'up',\n",
       "  'down',\n",
       "  'in',\n",
       "  'out',\n",
       "  'on',\n",
       "  'off',\n",
       "  'over',\n",
       "  'under',\n",
       "  'again',\n",
       "  'further',\n",
       "  'then',\n",
       "  'once',\n",
       "  'here',\n",
       "  'there',\n",
       "  'when',\n",
       "  'where',\n",
       "  'why',\n",
       "  'how',\n",
       "  'all',\n",
       "  'any',\n",
       "  'both',\n",
       "  'each',\n",
       "  'few',\n",
       "  'more',\n",
       "  'most',\n",
       "  'other',\n",
       "  'some',\n",
       "  'such',\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'only',\n",
       "  'own',\n",
       "  'same',\n",
       "  'so',\n",
       "  'than',\n",
       "  'too',\n",
       "  'very',\n",
       "  's',\n",
       "  't',\n",
       "  'can',\n",
       "  'will',\n",
       "  'just',\n",
       "  'don',\n",
       "  \"don't\",\n",
       "  'should',\n",
       "  \"should've\",\n",
       "  'now',\n",
       "  'd',\n",
       "  'll',\n",
       "  'm',\n",
       "  'o',\n",
       "  're',\n",
       "  've',\n",
       "  'y',\n",
       "  'ain',\n",
       "  'aren',\n",
       "  \"aren't\",\n",
       "  'couldn',\n",
       "  \"couldn't\",\n",
       "  'didn',\n",
       "  \"didn't\",\n",
       "  'doesn',\n",
       "  \"doesn't\",\n",
       "  'hadn',\n",
       "  \"hadn't\",\n",
       "  'hasn',\n",
       "  \"hasn't\",\n",
       "  'haven',\n",
       "  \"haven't\",\n",
       "  'isn',\n",
       "  \"isn't\",\n",
       "  'ma',\n",
       "  'mightn',\n",
       "  \"mightn't\",\n",
       "  'mustn',\n",
       "  \"mustn't\",\n",
       "  'needn',\n",
       "  \"needn't\",\n",
       "  'shan',\n",
       "  \"shan't\",\n",
       "  'shouldn',\n",
       "  \"shouldn't\",\n",
       "  'wasn',\n",
       "  \"wasn't\",\n",
       "  'weren',\n",
       "  \"weren't\",\n",
       "  'won',\n",
       "  \"won't\",\n",
       "  'wouldn',\n",
       "  \"wouldn't\"],\n",
       " 179)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words, len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of tokens 53\n",
      "Filtered words [\"Muad'Dib\", 'learned', 'rapidly', 'first', 'training', 'learn', '.', 'And', 'first', 'lesson', 'basic', 'trust', 'could', 'learn', '.', 'It', \"'s\", 'shocking', 'find', 'many', 'people', 'believe', 'learn', ',', 'many', 'believe', 'learning', 'difficult', '.']\n",
      "len of filter words 29\n"
     ]
    }
   ],
   "source": [
    "example_string = \"\"\"\n",
    "... Muad'Dib learned rapidly because his first training was in how to learn.\n",
    "... And the first lesson of all was the basic trust that he could learn.\n",
    "... It's shocking to find how many people do not believe they can learn,\n",
    "... and how many more believe learning to be difficult.\"\"\"\n",
    "ml_tokenize = nltk.word_tokenize(example_string)\n",
    "print(\"Total length of tokens\", len(ml_tokenize))\n",
    "filter_words = [token for token in ml_tokenize if not token in stop_words]\n",
    "print(\"Filtered words\", filter_words)\n",
    "print(\"len of filter words\", len(filter_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter stemmming method Generous --> gener\n",
      "lancaster stemmming method Generous --> gen\n",
      "snowball stemmming method Generous --> generous\n",
      "reges stemmming method Generous --> Generou\n",
      "\n",
      "porter stemmming method Generate --> gener\n",
      "lancaster stemmming method Generate --> gen\n",
      "snowball stemmming method Generate --> generat\n",
      "reges stemmming method Generate --> Generate\n",
      "\n",
      "porter stemmming method Genorously --> genor\n",
      "lancaster stemmming method Genorously --> gen\n",
      "snowball stemmming method Genorously --> genor\n",
      "reges stemmming method Genorously --> Genorously\n",
      "\n",
      "porter stemmming method Generation --> gener\n",
      "lancaster stemmming method Generation --> gen\n",
      "snowball stemmming method Generation --> generat\n",
      "reges stemmming method Generation --> Generation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Porter stemming\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, RegexpStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "snow_ball_stemmer = SnowballStemmer(language=\"english\")\n",
    "reges_stremmer = RegexpStemmer('ing|s$|ables&', min=4)\n",
    "# regex_stemmer = RegexpStemmer()\n",
    "\n",
    "words = [\"Generous\", \"Generate\", \"Genorously\", \"Generation\"]\n",
    "\n",
    "for word in words:\n",
    "    print(f\"porter stemmming method {word} --> {porter_stemmer.stem(word)}\")\n",
    "    print(f\"lancaster stemmming method {word} --> {lancaster_stemmer.stem(word)}\")\n",
    "    print(f\"snowball stemmming method {word} --> {snow_ball_stemmer.stem(word)}\")\n",
    "    print(f\"reges stemmming method {word} --> {reges_stremmer.stem(word)}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lematazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lematazation method for Generous --> Generous\n",
      "Lematazation method for Generate --> Generate\n",
      "Lematazation method for Genorously --> Genorously\n",
      "Lematazation method for Generation --> Generation\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "words = [\"Generous\", \"Generate\", \"Genorously\", \"Generation\"]\n",
    "\n",
    "lematazation = WordNetLemmatizer()\n",
    "for word in words:\n",
    "    print(f\"Lematazation method for {word} --> {lematazation.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE Apple\n"
     ]
    }
   ],
   "source": [
    "text = \"Apple is an american company based out of california\"\n",
    "\n",
    "for w in nltk.word_tokenize(text):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(w))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ''.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02') a container (usually with a slot in the top) for keeping money at home\n",
      "Synset('bank.v.07') cover with ashes so to control the rate of burning\n"
     ]
    }
   ],
   "source": [
    "a1 = lesk(word_tokenize(\"He deposited money in the bank\"), \"bank\")\n",
    "a2 = lesk(word_tokenize(\"He sat on the bank of the river\"), \"bank\")\n",
    "\n",
    "print(a1, a1.definition())\n",
    "print(a2, a2.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "Kohli was the captain of the 2008 U19 World Cup winning team and was a crucial member of the teams that won 2011 ODI World Cup, \n",
    "2013 Champions Trophy, 2024 T20 World Cup, and 2025 Champions Trophy. \n",
    "He plays for Royal Challengers Bengaluru in the Indian Premier League and for Delhi in domestic cricket. \n",
    "In 2013, Kohli was ranked number one in the ODI batting rankings. \n",
    "In 2015, he achieved the same in T20I. In 2018, he was ranked number one in Test, making him the only Indian to hold the number one spot in all three formats. \n",
    "He is the first player to score 20,000 runs in a decade. He was the Cricketer of the Decade for 2011 to 2020.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " Kohli was the captain of the 2008 U19 World Cup winning team and was a crucial member of the teams that won 2011 ODI World Cup, \n",
       " 2013 Champions Trophy, 2024 T20 World Cup, and 2025 Champions Trophy. \n",
       " He plays for Royal Challengers Bengaluru in the Indian Premier League and for Delhi in domestic cricket. \n",
       " In 2013, Kohli was ranked number one in the ODI batting rankings. \n",
       " In 2015, he achieved the same in T20I. In 2018, he was ranked number one in Test, making him the only Indian to hold the number one spot in all three formats. \n",
       " He is the first player to score 20,000 runs in a decade. He was the Cricketer of the Decade for 2011 to 2020.,\n",
       " spacy.tokens.doc.Doc)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(summary)\n",
    "doc, type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SPACE \n",
      "\n",
      "Kohli PROPN Kohli\n",
      "was AUX be\n",
      "the DET the\n",
      "captain NOUN captain\n",
      "of ADP of\n",
      "the DET the\n",
      "2008 NUM 2008\n",
      "U19 PROPN U19\n",
      "World PROPN World\n",
      "Cup PROPN Cup\n",
      "winning VERB win\n",
      "team NOUN team\n",
      "and CCONJ and\n",
      "was AUX be\n",
      "a DET a\n",
      "crucial ADJ crucial\n",
      "member NOUN member\n",
      "of ADP of\n",
      "the DET the\n",
      "teams NOUN team\n",
      "that PRON that\n",
      "won VERB win\n",
      "2011 NUM 2011\n",
      "ODI PROPN ODI\n",
      "World PROPN World\n",
      "Cup PROPN Cup\n",
      ", PUNCT ,\n",
      "\n",
      " SPACE \n",
      "\n",
      "2013 NUM 2013\n",
      "Champions PROPN Champions\n",
      "Trophy PROPN Trophy\n",
      ", PUNCT ,\n",
      "2024 NUM 2024\n",
      "T20 PROPN T20\n",
      "World PROPN World\n",
      "Cup PROPN Cup\n",
      ", PUNCT ,\n",
      "and CCONJ and\n",
      "2025 NUM 2025\n",
      "Champions PROPN Champions\n",
      "Trophy PROPN Trophy\n",
      ". PUNCT .\n",
      "\n",
      " SPACE \n",
      "\n",
      "He PRON he\n",
      "plays VERB play\n",
      "for ADP for\n",
      "Royal PROPN Royal\n",
      "Challengers PROPN Challengers\n",
      "Bengaluru PROPN Bengaluru\n",
      "in ADP in\n",
      "the DET the\n",
      "Indian ADJ indian\n",
      "Premier PROPN Premier\n",
      "League PROPN League\n",
      "and CCONJ and\n",
      "for ADP for\n",
      "Delhi PROPN Delhi\n",
      "in ADP in\n",
      "domestic ADJ domestic\n",
      "cricket NOUN cricket\n",
      ". PUNCT .\n",
      "\n",
      " SPACE \n",
      "\n",
      "In ADP in\n",
      "2013 NUM 2013\n",
      ", PUNCT ,\n",
      "Kohli PROPN Kohli\n",
      "was AUX be\n",
      "ranked VERB rank\n",
      "number NOUN number\n",
      "one NUM one\n",
      "in ADP in\n",
      "the DET the\n",
      "ODI PROPN ODI\n",
      "batting NOUN batting\n",
      "rankings NOUN ranking\n",
      ". PUNCT .\n",
      "\n",
      " SPACE \n",
      "\n",
      "In ADP in\n",
      "2015 NUM 2015\n",
      ", PUNCT ,\n",
      "he PRON he\n",
      "achieved VERB achieve\n",
      "the DET the\n",
      "same ADJ same\n",
      "in ADP in\n",
      "T20I. NOUN t20i.\n",
      "In ADP in\n",
      "2018 NUM 2018\n",
      ", PUNCT ,\n",
      "he PRON he\n",
      "was AUX be\n",
      "ranked VERB rank\n",
      "number NOUN number\n",
      "one NUM one\n",
      "in ADP in\n",
      "Test PROPN Test\n",
      ", PUNCT ,\n",
      "making VERB make\n",
      "him PRON he\n",
      "the DET the\n",
      "only ADJ only\n",
      "Indian ADJ indian\n",
      "to PART to\n",
      "hold VERB hold\n",
      "the DET the\n",
      "number NOUN number\n",
      "one NUM one\n",
      "spot NOUN spot\n",
      "in ADP in\n",
      "all DET all\n",
      "three NUM three\n",
      "formats NOUN format\n",
      ". PUNCT .\n",
      "\n",
      " SPACE \n",
      "\n",
      "He PRON he\n",
      "is AUX be\n",
      "the DET the\n",
      "first ADJ first\n",
      "player NOUN player\n",
      "to PART to\n",
      "score VERB score\n",
      "20,000 NUM 20,000\n",
      "runs NOUN run\n",
      "in ADP in\n",
      "a DET a\n",
      "decade NOUN decade\n",
      ". PUNCT .\n",
      "He PRON he\n",
      "was AUX be\n",
      "the DET the\n",
      "Cricketer PROPN Cricketer\n",
      "of ADP of\n",
      "the DET the\n",
      "Decade NOUN decade\n",
      "for ADP for\n",
      "2011 NUM 2011\n",
      "to ADP to\n",
      "2020 NUM 2020\n",
      ". PUNCT .\n",
      "\n",
      " SPACE \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'proper noun'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"PROPN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kohli PERSON\n",
      "2008 DATE\n",
      "U19 World Cup EVENT\n",
      "2011 DATE\n",
      "World Cup EVENT\n",
      "2013 DATE\n",
      "Champions Trophy PERSON\n",
      "2024 DATE\n",
      "T20 World Cup EVENT\n",
      "2025 DATE\n",
      "Royal Challengers Bengaluru ORG\n",
      "Indian NORP\n",
      "Delhi GPE\n",
      "2013 DATE\n",
      "Kohli PERSON\n",
      "one CARDINAL\n",
      "ODI ORG\n",
      "2015 DATE\n",
      "2018 DATE\n",
      "one CARDINAL\n",
      "Indian NORP\n",
      "three CARDINAL\n",
      "first ORDINAL\n",
      "20,000 CARDINAL\n",
      "a decade DATE\n",
      "the Cricketer of PERSON\n",
      "2011 DATE\n",
      "2020 DATE\n"
     ]
    }
   ],
   "source": [
    "## Extracting the entities\n",
    "for entities in doc.ents:\n",
    "    print(entities.text, entities.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
